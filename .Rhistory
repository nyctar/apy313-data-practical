filter(n >= 7) %>%
graph_from_data_frame()
# Plot graph
ggraph(word_graph, layout = "fr") +
geom_edge_link(aes(edge_alpha = n), show.legend = FALSE) +
geom_node_point(color = "lightblue", size = 5) +
geom_node_text(aes(label = name), repel = TRUE) +
theme_void() +
labs(title = "Word Co-occurrence Network for Journal 2")
# Bigram graph for Journal 1
# Tutorial: https://s-ai-f.github.io/Natural-Language-Processing/words-relationships-analysis.html#visualizing-a-network-of-bigrams
data("stop_words", package = "tidytext")
# Create bigrams from Journal1
bigrams <- tb %>%
filter(journal == "Journal1") %>%
unnest_tokens(bigram, text, token = "ngrams", n = 2) %>%
separate(bigram, into = c("word1", "word2"), sep = " ") %>%
filter(!is.na(word1) & !is.na(word2)) %>%
filter(!word1 %in% stop_words$word & !word2 %in% stop_words$word) %>%
filter(!str_detect(word1, "\\d") & !str_detect(word2, "\\d")) %>%
count(word1, word2, sort = TRUE) %>%
filter(n >= 3)  # adjust threshold for readability
bigram_graph <- graph_from_data_frame(bigrams)
ggraph(bigram_graph, layout = "fr") +
geom_edge_link(aes(edge_alpha = n), show.legend = FALSE) +
geom_node_point(color = "lightblue", size = 5) +
geom_node_text(aes(label = name), repel = TRUE) +
theme_void() +
labs(title = "Bigram Network Graph for Journal 1")
# Bigram graph for Journal 1
# Tutorial: https://s-ai-f.github.io/Natural-Language-Processing/words-relationships-analysis.html#visualizing-a-network-of-bigrams
data("stop_words", package = "tidytext")
# Create bigrams from Journal1
bigrams <- tb %>%
filter(journal == "Journal2") %>%
unnest_tokens(bigram, text, token = "ngrams", n = 2) %>%
separate(bigram, into = c("word1", "word2"), sep = " ") %>%
filter(!is.na(word1) & !is.na(word2)) %>%
filter(!word1 %in% stop_words$word & !word2 %in% stop_words$word) %>%
filter(!str_detect(word1, "\\d") & !str_detect(word2, "\\d")) %>%
count(word1, word2, sort = TRUE) %>%
filter(n >= 3)  # adjust threshold for readability
bigram_graph <- graph_from_data_frame(bigrams)
ggraph(bigram_graph, layout = "fr") +
geom_edge_link(aes(edge_alpha = n), show.legend = FALSE) +
geom_node_point(color = "lightblue", size = 5) +
geom_node_text(aes(label = name), repel = TRUE) +
theme_void() +
labs(title = "Bigram Network Graph for Journal 2")
locations <- c("Philadelphia", "Tampico", "Siera Madre", "San Louis Potosi", "San Maria del Lagos", "Tepatillan", "Gaudalaxhara", "Tekelah",
"Tepec", "Cauponetta", "Rosario", "Mazatlan")
# Create a regex pattern from the locations vector
location_pattern <- str_c("\\b(", str_c(locations, collapse = "|"), ")\\b")
# Extract location (first match only)
tb_located <- tb %>%
mutate(location = str_extract_all(text, regex(location_pattern, ignore_case = TRUE))) %>%
filter(lengths(location) > 0) %>%
unnest(location)
tb_located
data("stop_words")
nrc <- get_sentiments("nrc")
peak_emotion_by_location <- tb_located %>%
unnest_tokens(word, text) %>%
anti_join(stop_words, by = "word") %>%
inner_join(nrc, by = "word") %>%
filter(!sentiment %in% c("positive", "negative")) %>%
count(location, sentiment, sort = TRUE) %>%
group_by(location) %>%
slice_max(n, n = 1, with_ties = FALSE) %>%
ungroup()
# All emotions per location, sorted from highest to lowest
emotion_ranking_by_location <- tb_located %>%
unnest_tokens(word, text) %>%
anti_join(stop_words, by = "word") %>%
inner_join(nrc, by = "word") %>%
filter(!sentiment %in% c("positive", "negative")) %>%
count(location, sentiment, sort = TRUE) %>%
arrange(location, desc(n))
ggplot(emotion_ranking_by_location, aes(x = fct_reorder(sentiment, n), y = n, fill = sentiment)) +
geom_col(show.legend = FALSE) +
facet_wrap(~ location, scales = "free_y") +
coord_flip() +
theme_minimal() +
labs(title = "Top NRC Emotions by Location",
x = "Emotion", y = "Word Count")
ggplot(emotion_ranking_by_location, aes(x = fct_reorder(location, n), y = n, fill = sentiment)) +
geom_col(position = "stack") +
coord_flip() +
theme_minimal() +
labs(title = "Emotion Composition by Location",
x = "Location", y = "Emotion Word Count", fill = "Emotion")
top3_emotions_by_location <- emotion_ranking_by_location %>%
group_by(location) %>%
slice_max(n, n = 3, with_ties = FALSE) %>%
ungroup()
ggplot(emotion_paragraphs_by_location, aes(x = fct_reorder(location, n), y = n, fill = sentiment)) +
geom_col(position = "dodge") +
coord_flip() +
theme_minimal() +
labs(title = "Top 3 Emotions by Location",
x = "Location", y = "Emotion Word Count", fill = "Emotion")
library(glue)
top_emotion_by_location <- emotion_ranking_by_location %>%
group_by(location) %>%
slice_max(n, n = 1, with_ties = FALSE) %>%
ungroup()
emotion_paragraphs_top1 <- top_emotion_by_location %>%
mutate(paragraph = map_chr(location, ~ {
tb_located %>%
filter(location == .x) %>%
pull(text) %>%
str_c(collapse = " ")
}))
emotion_paragraphs_top1 %>%
arrange(location, desc(n)) %>%
pwalk(function(location, sentiment, n, paragraph) {
cat(glue("### {location} â€” {sentiment} ({n} words)\n\n"))
cat(paragraph)
cat("\n\n---\n\n")
})
tb_nrc <- tb %>%
unnest_tokens(word, text) %>%
inner_join(nrc, by = "word") %>%
filter(!sentiment %in% c("positive", "negative")) %>%
mutate(index_by_10 = ceiling(row_number() / 10))
nrc_by_index <- tb_nrc %>%
group_by(sentiment, index_by_10) %>%
summarise(count = n(), .groups = "drop")
nrc_peak_paragraphs <- tb_nrc %>%
group_by(sentiment, index_by_10) %>%
summarise(count = n(), .groups = "drop") %>%
group_by(sentiment) %>%
slice_max(count, n = 1, with_ties = FALSE) %>%
rowwise() %>%
mutate(paragraph = tb %>%
filter(index_by_10 == index_by_10) %>%
pull(text) %>%
str_c(collapse = " ")) %>%
ungroup()
# Final structure: list of emotions
export_data <- nrc_by_index %>%
group_by(sentiment) %>%
nest(data = c(index_by_10, count)) %>%
left_join(nrc_peak_paragraphs %>% select(sentiment, peak_index = index_by_10, peak_count = count, paragraph), by = "sentiment")
write_json(export_data, "nrc_emotions_over_time.json", pretty = TRUE)
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(tidytext)
library(textdata)
library(wordcloud)
library(widyr)
library(igraph)
library(ggraph)
library(topicmodels)
library(spacyr)
library(tokenizers)
library(jsonlite)
library(reshape2)
library(forcats)
# Read files
journal1_lines <- readLines("Journal1.txt")
journal2_lines <- readLines("Journal2.txt")
# Put into dataframe
df1 <- data.frame(journal = "Journal1", text = journal1_lines, stringsAsFactors = FALSE)
df2 <- data.frame(journal = "Journal2", text = journal2_lines, stringsAsFactors = FALSE)
combined_journals <- rbind(df1, df2)
combined_journals
# Add an index every n lines for a more detailed analysis
tb <- as_tibble(combined_journals)
tb  <- combined_journals %>%
mutate(line_number = row_number()) %>%
mutate(index_by_10 = ((line_number) %/% 10) + 1) %>%
mutate(index_by_20 = ((line_number) %/% 20) + 1)
head(tb)
# Get sentiment
bing <- tidytext::get_sentiments("bing") %>%
distinct(word, .keep_all = TRUE)
afinn <- tidytext::get_sentiments("afinn") %>%
distinct(word, .keep_all = TRUE)
nrc <- tidytext::get_sentiments("nrc") %>%
distinct(word, .keep_all = TRUE)
# Tokenize and clean data
tb_bing <- tb %>%
unnest_tokens(word,text) %>%
anti_join(tidytext::stop_words, by = "word") %>%
filter(!str_detect(word, "\\d")) %>%
inner_join(bing, by = "word")
tb_nrc <- tb %>%
unnest_tokens(word,text) %>%
anti_join(tidytext::stop_words, by = "word") %>%
filter(!str_detect(word, "\\d")) %>%
inner_join(nrc, by = "word")
tb_afinn <- tb %>%
unnest_tokens(word,text) %>%
anti_join(tidytext::stop_words, by = "word") %>%
filter(!str_detect(word, "\\d")) %>%
inner_join(afinn, by = "word")
tb_bing %>%
count(sentiment, sort = TRUE)
tb_nrc %>%
count(sentiment, sort = TRUE)
tb_afinn %>%
summarize(overall_score = sum(value, na.rm = TRUE))
tb_clean <- tb %>%
unnest_tokens(word,text) %>%
anti_join(tidytext::stop_words, by = "word") %>%
filter(!str_detect(word, "\\d"))
tb_clean %>%
count(word, sort = TRUE) %>%
with(wordcloud(word, n, max.words = 50, colors = brewer.pal(8, "Dark2")))
# clean text and tokenize
tb_clean <- tb %>%
unnest_tokens(word, text) %>%
anti_join(tidytext::stop_words, by = "word") %>%
filter(!str_detect(word, "\\d"))
# Join sentiment
sentiment_counts <- tb_clean %>%
inner_join(get_sentiments("bing"), by = "word") %>%
count(word, sentiment, sort = TRUE) %>%
acast(word ~ sentiment, value.var = "n", fill = 0)
# Plot word cloud
comparison.cloud(sentiment_counts,
colors = c("red", "green"),
max.words = 100)
# Bag of Words
bag_of_words <- tb_clean %>%
count(word, sort = TRUE)
head(bag_of_words, 20)
tb_clean <- tb %>%
unnest_tokens(word, text) %>%
anti_join(tidytext::stop_words, by = "word") %>%
filter(!str_detect(word, "\\d")) %>%
count(word, sort = TRUE) %>%
rename(text = word, size = n) %>%
head(50)
# JSON file
write_json(tb_clean, "word-cloud.json", pretty = TRUE)
tb_bing_sentiment_bar_plot <- tb_bing %>%
count(word, sentiment, sort = TRUE) %>%
group_by(sentiment) %>%
slice_max(n, n = 10) %>%
ungroup() %>%
mutate(word = reorder(word,n))
ggplot(tb_bing_sentiment_bar_plot, aes(n, word, fill = sentiment)) +
geom_col(show.legend = FALSE) +
facet_wrap(~sentiment, scales = "free_y") +
theme_minimal() +
labs(title = "Negative/Positive Words in the Journal")
# Sentiment breakdown for each journal
tb_bing_sentiment_by_journal <- tb_bing %>%
count(journal, sentiment)
ggplot(tb_bing_sentiment_by_journal, aes(x = journal, y = n, fill = sentiment)) +
geom_bar(stat = "identity", position = "stack") +
theme_minimal() +
labs(title = "Sentiment Breakdown by Journal", x = "Journal", y = "Count")
text_df_index10_score <- tb_afinn %>%
group_by(index_by_10) %>%
summarize(sentiment = sum(value)) %>%
ungroup() %>%
mutate(positive = sentiment > 0)
# JSON file
write_json(text_df_index10_score, "sentiment-bar-chart.json", pretty = TRUE)
ggplot(text_df_index10_score, aes(index_by_10, sentiment)) +
geom_col(aes(fill = positive), show.legend = FALSE) +
theme_minimal() +
theme(panel.grid.major.x = element_blank(),
panel.grid.minor.x = element_blank()) +
labs(title = "Sentiment analysis of the Journals by 10 Lines")
# AFINN: Group by index and sum sentiment
afinn_peaks <- tb_afinn %>%
group_by(index_by_10) %>%
summarise(sentiment = sum(value), .groups = "drop")
# Most positive and most negative
most_positive <- afinn_peaks %>% slice_max(sentiment, n = 1)
most_negative <- afinn_peaks %>% slice_min(sentiment, n = 1)
most_positive
most_negative
# Pull lines for the most positive group
positive_lines <- tb %>%
filter(index_by_10 == most_positive$index_by_10)
# Combine the text into one paragraph
positive_paragraph <- positive_lines %>%
pull(text) %>%
str_c(collapse = " ")
positive_paragraph
# Pull lines for the most negative group
negative_lines <- tb %>%
filter(index_by_10 == most_negative$index_by_10)
negative_lines
# Combine the text into one paragraph
negative_paragraph <- negative_lines %>%
pull(text) %>%
str_c(collapse = " ")
negative_paragraph
# Emotion breakdown for each journal
tb_nrc_emotion_by_journal <- tb_nrc %>%
count(journal, sentiment)
ggplot(tb_nrc_emotion_by_journal, aes(x = journal, y = n, fill = sentiment)) +
geom_bar(stat = "identity", position = "stack") +
theme_minimal() +
labs(title = "Emotion Breakdown by Journal", x = "Journal", y = "Count")
tb_nrc_sentiment_bar_plot <- tb_nrc %>%
count(word,, sentiment, sort = TRUE) %>%
group_by(sentiment) %>%
slice_max(n, n = 5) %>%
ungroup() %>%
mutate(word = reorder(word,n))
ggplot(tb_nrc_sentiment_bar_plot, aes(n, word, fill = sentiment)) +
geom_col(show.legend = FALSE) +
facet_wrap(~sentiment, scales = "free_y") +
theme_minimal() +
labs(title = "Emotional Words in the Journal")
# Group NRC sentiments by index and emotion
nrc_index10_emotions <- tb_nrc %>%
group_by(index_by_10, sentiment) %>%
summarise(count = n(), .groups = "drop")
# Plot result
ggplot(nrc_index10_emotions, aes(x = index_by_10, y = count, fill = sentiment)) +
geom_col(position = "stack") +
facet_wrap(~ sentiment, scales = "free_y") +
theme_minimal() +
labs(title = "NRC Emotions by 10 Lines")
# Get peak index_by_10 for each NRC emotion
nrc_peaks <- tb_nrc %>%
group_by(sentiment, index_by_10) %>%
summarise(count = n(), .groups = "drop") %>%
group_by(sentiment) %>%
slice_max(count, n = 1, with_ties = FALSE)
# Create a dataframe of peak emotion paragraphs
emotion_paragraphs <- nrc_peaks %>%
mutate(paragraph = map_chr(index_by_10, ~ {
tb %>%
filter(index_by_10 == .x) %>%
pull(text) %>%
str_c(collapse = " ")
}))
emotion_paragraphs %>%
filter(sentiment == "anger") %>%
pull(paragraph)
tb_clean <- tb %>%
unnest_tokens(word, text) %>%
anti_join(tidytext::stop_words, by = "word") %>%
filter(!str_detect(word, "\\d"))
# Get pairwise word co-occurrence within the same journal line
word_pairs <- tb_clean %>%
filter(journal == "Journal1") %>%
pairwise_count(word, line_number, sort = TRUE, upper = FALSE)
# Create graph from word pairs
word_graph <- word_pairs %>%
filter(n >= 7) %>%
graph_from_data_frame()
# Plot graph
ggraph(word_graph, layout = "fr") +
geom_edge_link(aes(edge_alpha = n), show.legend = FALSE) +
geom_node_point(color = "lightblue", size = 5) +
geom_node_text(aes(label = name), repel = TRUE) +
theme_void() +
labs(title = "Word Co-occurrence Network for Journal 1")
# Get pairwise word co-occurrence within the same journal line
word_pairs <- tb_clean %>%
filter(journal == "Journal2") %>%
pairwise_count(word, line_number, sort = TRUE, upper = FALSE)
# Create graph from word pairs
word_graph <- word_pairs %>%
filter(n >= 7) %>%
graph_from_data_frame()
# Plot graph
ggraph(word_graph, layout = "fr") +
geom_edge_link(aes(edge_alpha = n), show.legend = FALSE) +
geom_node_point(color = "lightblue", size = 5) +
geom_node_text(aes(label = name), repel = TRUE) +
theme_void() +
labs(title = "Word Co-occurrence Network for Journal 2")
# Bigram graph for Journal 1
# Tutorial: https://s-ai-f.github.io/Natural-Language-Processing/words-relationships-analysis.html#visualizing-a-network-of-bigrams
data("stop_words", package = "tidytext")
# Create bigrams from Journal1
bigrams <- tb %>%
filter(journal == "Journal1") %>%
unnest_tokens(bigram, text, token = "ngrams", n = 2) %>%
separate(bigram, into = c("word1", "word2"), sep = " ") %>%
filter(!is.na(word1) & !is.na(word2)) %>%
filter(!word1 %in% stop_words$word & !word2 %in% stop_words$word) %>%
filter(!str_detect(word1, "\\d") & !str_detect(word2, "\\d")) %>%
count(word1, word2, sort = TRUE) %>%
filter(n >= 3)  # adjust threshold for readability
bigram_graph <- graph_from_data_frame(bigrams)
ggraph(bigram_graph, layout = "fr") +
geom_edge_link(aes(edge_alpha = n), show.legend = FALSE) +
geom_node_point(color = "lightblue", size = 5) +
geom_node_text(aes(label = name), repel = TRUE) +
theme_void() +
labs(title = "Bigram Network Graph for Journal 1")
# Bigram graph for Journal 1
# Tutorial: https://s-ai-f.github.io/Natural-Language-Processing/words-relationships-analysis.html#visualizing-a-network-of-bigrams
data("stop_words", package = "tidytext")
# Create bigrams from Journal1
bigrams <- tb %>%
filter(journal == "Journal2") %>%
unnest_tokens(bigram, text, token = "ngrams", n = 2) %>%
separate(bigram, into = c("word1", "word2"), sep = " ") %>%
filter(!is.na(word1) & !is.na(word2)) %>%
filter(!word1 %in% stop_words$word & !word2 %in% stop_words$word) %>%
filter(!str_detect(word1, "\\d") & !str_detect(word2, "\\d")) %>%
count(word1, word2, sort = TRUE) %>%
filter(n >= 3)  # adjust threshold for readability
bigram_graph <- graph_from_data_frame(bigrams)
ggraph(bigram_graph, layout = "fr") +
geom_edge_link(aes(edge_alpha = n), show.legend = FALSE) +
geom_node_point(color = "lightblue", size = 5) +
geom_node_text(aes(label = name), repel = TRUE) +
theme_void() +
labs(title = "Bigram Network Graph for Journal 2")
locations <- c("Philadelphia", "Tampico", "Siera Madre", "San Louis Potosi", "San Maria del Lagos", "Tepatillan", "Gaudalaxhara", "Tekelah",
"Tepec", "Cauponetta", "Rosario", "Mazatlan")
# Create a regex pattern from the locations vector
location_pattern <- str_c("\\b(", str_c(locations, collapse = "|"), ")\\b")
# Extract location (first match only)
tb_located <- tb %>%
mutate(location = str_extract_all(text, regex(location_pattern, ignore_case = TRUE))) %>%
filter(lengths(location) > 0) %>%
unnest(location)
tb_located
data("stop_words")
nrc <- get_sentiments("nrc")
peak_emotion_by_location <- tb_located %>%
unnest_tokens(word, text) %>%
anti_join(stop_words, by = "word") %>%
inner_join(nrc, by = "word") %>%
filter(!sentiment %in% c("positive", "negative")) %>%
count(location, sentiment, sort = TRUE) %>%
group_by(location) %>%
slice_max(n, n = 1, with_ties = FALSE) %>%
ungroup()
# All emotions per location, sorted from highest to lowest
emotion_ranking_by_location <- tb_located %>%
unnest_tokens(word, text) %>%
anti_join(stop_words, by = "word") %>%
inner_join(nrc, by = "word") %>%
filter(!sentiment %in% c("positive", "negative")) %>%
count(location, sentiment, sort = TRUE) %>%
arrange(location, desc(n))
ggplot(emotion_ranking_by_location, aes(x = fct_reorder(sentiment, n), y = n, fill = sentiment)) +
geom_col(show.legend = FALSE) +
facet_wrap(~ location, scales = "free_y") +
coord_flip() +
theme_minimal() +
labs(title = "Top NRC Emotions by Location",
x = "Emotion", y = "Word Count")
ggplot(emotion_ranking_by_location, aes(x = fct_reorder(location, n), y = n, fill = sentiment)) +
geom_col(position = "stack") +
coord_flip() +
theme_minimal() +
labs(title = "Emotion Composition by Location",
x = "Location", y = "Emotion Word Count", fill = "Emotion")
top3_emotions_by_location <- emotion_ranking_by_location %>%
group_by(location) %>%
slice_max(n, n = 3, with_ties = FALSE) %>%
ungroup()
ggplot(emotion_paragraphs_by_location, aes(x = fct_reorder(location, n), y = n, fill = sentiment)) +
geom_col(position = "dodge") +
coord_flip() +
theme_minimal() +
labs(title = "Top 3 Emotions by Location",
x = "Location", y = "Emotion Word Count", fill = "Emotion")
library(glue)
top_emotion_by_location <- emotion_ranking_by_location %>%
group_by(location) %>%
slice_max(n, n = 1, with_ties = FALSE) %>%
ungroup()
emotion_paragraphs_top1 <- top_emotion_by_location %>%
mutate(paragraph = map_chr(location, ~ {
tb_located %>%
filter(location == .x) %>%
pull(text) %>%
str_c(collapse = " ")
}))
emotion_paragraphs_top1 %>%
arrange(location, desc(n)) %>%
pwalk(function(location, sentiment, n, paragraph) {
cat(glue("### {location} â€” {sentiment} ({n} words)\n\n"))
cat(paragraph)
cat("\n\n---\n\n")
})
tb_nrc <- tb %>%
unnest_tokens(word, text) %>%
inner_join(nrc, by = "word") %>%
filter(!sentiment %in% c("positive", "negative")) %>%
mutate(index_by_10 = ceiling(row_number() / 10))
nrc_by_index <- tb_nrc %>%
group_by(sentiment, index_by_10) %>%
summarise(count = n(), .groups = "drop")
nrc_peak_paragraphs <- tb_nrc %>%
group_by(sentiment, index_by_10) %>%
summarise(count = n(), .groups = "drop") %>%
group_by(sentiment) %>%
slice_max(count, n = 1, with_ties = FALSE) %>%
rowwise() %>%
mutate(paragraph = tb %>%
filter(index_by_10 == index_by_10) %>%
pull(text) %>%
str_c(collapse = " ")) %>%
ungroup()
# Final structure: list of emotions
export_data <- nrc_by_index %>%
group_by(sentiment) %>%
nest(data = c(index_by_10, count)) %>%
left_join(nrc_peak_paragraphs %>% select(sentiment, peak_index = index_by_10, peak_count = count, paragraph), by = "sentiment")
write_json(export_data, "nrc_emotions_over_time.json", pretty = TRUE)
tb
knitr::opts_knit$set(echo = TRUE, output.dir = "docs")
tb
