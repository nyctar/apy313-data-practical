ggraph(graph, layout = "fr") +
geom_edge_link(aes(edge_alpha = n), show.legend = FALSE) +
geom_node_point(color = "skyblue", size = 5) +
geom_node_text(aes(label = name), repel = TRUE, size = 4) +
labs(title = "Afternoon Word Co-Occurrence Network") +
theme_void()
# Filter evening and remove stop words
evening_words <- time_balanced %>%
filter(time_of_day == "Evening") %>%
select(row_id, text) %>%
unnest_tokens(word, text) %>%
filter(!word %in% expanded_stop_words,
hunspell_check(word, dict = "en_US")) #stop word filter
# Calculate co-occurrence counts (within same post)
evening_pairs <- evening_words %>%
pairwise_count(word, row_id, sort = TRUE, upper = FALSE)
# Filter for strong connections only
evening_pairs_filtered <- evening_pairs %>%
filter(n >= 3)  # Only keep strong links
# Build graph
graph <- evening_pairs_filtered %>%
graph_from_data_frame()
# Plot with ggraph
ggraph(graph, layout = "fr") +
geom_edge_link(aes(edge_alpha = n), show.legend = FALSE) +
geom_node_point(color = "skyblue", size = 5) +
geom_node_text(aes(label = name), repel = TRUE, size = 4) +
labs(title = "Evening Word Co-Occurrence Network") +
theme_void()
# Filter night and remove stop words
night_words <- time_balanced %>%
filter(time_of_day == "Night") %>%
select(row_id, text) %>%
unnest_tokens(word, text) %>%
filter(!word %in% expanded_stop_words,
hunspell_check(word, dict = "en_US")) #stop word filter
# Calculate co-occurrence counts (within same post)
night_pairs <- night_words %>%
pairwise_count(word, row_id, sort = TRUE, upper = FALSE)
# Filter for strong connections only
night_pairs_filtered <- night_pairs %>%
filter(n >= 3)  # Only keep strong links
# Build graph
graph <- night_pairs_filtered %>%
graph_from_data_frame()
# Plot with ggraph
ggraph(graph, layout = "fr") +
geom_edge_link(aes(edge_alpha = n), show.legend = FALSE) +
geom_node_point(color = "skyblue", size = 5) +
geom_node_text(aes(label = name), repel = TRUE, size = 4) +
labs(title = "Night Word Co-Occurrence Network") +
theme_void()
# Function to compute network
network_stats_by_time <- function(data, time_label) {
# Filter for one time_of_day group
word_pairs <- data %>%
filter(time_of_day == time_label) %>%
unnest_tokens(word, text) %>%
filter(!word %in% expanded_stop_words,
hunspell_check(word, dict = "en_US")) %>%
pairwise_count(word, row_id, sort = TRUE, upper = FALSE) %>%
filter(n >= 3)
# Build graph
g <- graph_from_data_frame(word_pairs, directed = FALSE)
# Compute metrics
data.frame(
time_of_day = time_label,
nodes = vcount(g),
edges = ecount(g),
density = edge_density(g),
components = components(g)$no,
modularity = modularity(cluster_louvain(g))
)
}
# Apply to all 4 time bins
network_summary <- bind_rows(
network_stats_by_time(time_balanced, "Morning"),
network_stats_by_time(time_balanced, "Afternoon"),
network_stats_by_time(time_balanced, "Evening"),
network_stats_by_time(time_balanced, "Night")
)
network_summary
# Bar chart of modularity
ggplot(network_summary, aes(x = factor(time_of_day, levels = c("Morning", "Afternoon", "Evening", "Night")),
y = modularity, fill = time_of_day)) +
geom_col(show.legend = FALSE) +
labs(title = "Topic Modularity by Time of Day",
x = "Time of Day",
y = "Modularity Score") +
theme_minimal()
# Build co-occurrence network per time bin
build_cooc_graph <- function(df, time_label) {
word_pairs <- df %>%
filter(time_of_day == time_label) %>%
unnest_tokens(word, text) %>%
filter(!word %in% expanded_stop_words,
hunspell_check(word, dict = "en_US")) %>%
pairwise_count(word, row_id, sort = TRUE, upper = FALSE) %>%
filter(n >= 3)
g <- graph_from_data_frame(word_pairs, directed = FALSE)
g <- simplify(g, remove.multiple = TRUE, remove.loops = TRUE)
# Add Louvain communities
V(g)$community <- cluster_louvain(g)$membership
V(g)$name <- V(g)$name
g
}
# Build graphs
g_morning   <- build_cooc_graph(time_balanced, "Morning")
g_afternoon <- build_cooc_graph(time_balanced, "Afternoon")
g_evening   <- build_cooc_graph(time_balanced, "Evening")
g_night     <- build_cooc_graph(time_balanced, "Night")
# Plot function
plot_network <- function(graph, title) {
ggraph(graph, layout = "fr") +
geom_edge_link(alpha = 0.2) +
geom_node_point(aes(color = as.factor(community)), size = 3) +
geom_node_text(aes(label = name), repel = TRUE, size = 3) +
labs(title = title) +
theme_void() +
theme(legend.position = "none")
}
# Generate plots
p1 <- plot_network(g_morning, "Morning")
p2 <- plot_network(g_afternoon, "Afternoon")
p3 <- plot_network(g_evening, "Evening")
p4 <- plot_network(g_night, "Night")
# Combine plots into a 2x2 grid
(p1 | p2) / (p3 | p4)
knitr::opts_knit$set(echo = TRUE, output.dir = "docs")
library(tidyverse)
library(reticulate)
library(cld2)
library(tidytext)
library(wordcloud)
library(word2vec)
library(text2vec)
library(Rtsne)
library(plotly)
library(reshape2)
library(topicmodels)
library(igraph)
library(ggraph)
library(quanteda)
library(rvest)
library(hunspell)
library(textTinyR)
library(httr)
library(tm)
library(reshape2)
library(widyr)
library(FactoMineR)
library(factoextra)
library(patchwork)
# Read CSV file into a tibble
tb<- read_csv("bluesky_data_VADER.csv")
# Sample only a portion of the data
# set.seed(123)  # For reproducibility
# tb <- tb %>% sample_n(100000)
# View first few rows
head(tb)
# Get time
tb <- tb %>%
mutate(
created_at = ymd_hms(created_at),  # Convert to datetime format
date = as.Date(created_at),        # Extract date
time = format(created_at, "%H:%M:%S") # Extract time
) %>%
filter(date >= as.Date("2024-01-01")) # Only get 2024 posts
knitr::opts_knit$set(echo = TRUE, output.dir = "docs")
library(tidyverse)
library(reticulate)
library(cld2)
library(tidytext)
library(wordcloud)
library(word2vec)
library(text2vec)
library(Rtsne)
library(plotly)
library(reshape2)
library(topicmodels)
library(igraph)
library(ggraph)
library(quanteda)
library(rvest)
library(hunspell)
library(textTinyR)
library(httr)
library(tm)
library(reshape2)
library(widyr)
library(FactoMineR)
library(factoextra)
library(patchwork)
# Read CSV file into a tibble
tb<- read_csv("bluesky_data_VADER.csv")
# Sample only a portion of the data
# set.seed(123)  # For reproducibility
# tb <- tb %>% sample_n(100000)
# View first few rows
head(tb)
# Get time
tb <- tb %>%
mutate(
created_at = ymd_hms(created_at),  # Convert to datetime format
date = as.Date(created_at),        # Extract date
time = format(created_at, "%H:%M:%S") # Extract time
) %>%
filter(date >= as.Date("2024-01-01")) # Only get 2024 posts
tb %>%
arrange(created_at) %>%
arrange(desc(created_at))
# Detect language and add to a column
tb <- tb %>%
mutate(language = cld2::detect_language(text)) %>%
filter(language == "en") # Only get English posts
head(tb)
tb <- tb %>%
mutate(
text = str_replace_all(text, "http[s]?://\\S+", ""),   # Remove URLs
text = str_replace_all(text, "@\\w+", ""),             # Remove mentions
text = str_replace_all(text, "#\\w+", ""),             # Remove hashtags
text = str_replace_all(text, "[^\x01-\x7F]", ""),      # Remove all non-ASCII characters (including emojis)
text = str_replace_all(text, "[[:punct:]]", ""),        # Remove punctuation
text = str_replace_all(text, "\\d+", "")               # Remove numbers
)
head(tb)
# Load stop words and convert to lowercase
data("stop_words")
stop_words <- tolower(stop_words$word)
# Remove apostrophes from stop words and create a new list with both forms
expanded_stop_words <- stop_words %>%
str_replace_all("'", "") %>%  # Remove apostrophes
unique()  # Get unique words
# Add the original stop words with apostrophes back to the list
expanded_stop_words <- unique(c(expanded_stop_words, stop_words))
# View the expanded stop words list
head(expanded_stop_words)
# NOTE: This list will be used during the tokenization process for the data visualization since the VADER Sentiment Lexicon can handle stop words
tb <- tb %>%
mutate(
hour = lubridate::hour(created_at),
weekday = weekdays(created_at),
weekday = factor(weekday, levels = c(
"Monday", "Tuesday", "Wednesday", "Thursday", "Friday", "Saturday", "Sunday"
)),
time_of_day = case_when(
hour >= 0 & hour < 6  ~ "Night",
hour >= 6 & hour < 12 ~ "Morning",
hour >= 12 & hour < 18 ~ "Afternoon",
hour >= 18 & hour < 24 ~ "Evening"
),
time_of_day = factor(time_of_day, levels = c("Morning", "Afternoon", "Evening", "Night"))
)
head(tb)
tb %>%
mutate(hour = hour(ymd_hms(created_at))) %>%
count(hour) %>%
ggplot(aes(x = hour, y = n)) +
geom_col(fill = "steelblue") +
scale_x_continuous(breaks = 0:23) +
labs(title = "Distribution of Posts by Hour",
x = "Hour of Day (24-hour format)",
y = "Number of Posts") +
theme_minimal()
weekday_counts <- tb %>%
count(weekday) %>%
mutate(percentage = round(100 * n / sum(n), 2))
ggplot(weekday_counts, aes(x = weekday, y = n)) +
geom_bar(stat = "identity", fill = "#F8766D") +
labs(title = "Post Count by Weekday",
x = "Weekday",
y = "Number of Posts") +
theme_minimal()
set.seed(123)
min_n_time <- tb %>%
count(time_of_day) %>%
pull(n) %>%
min()
time_balanced <- tb %>%
group_by(time_of_day) %>%
slice_sample(n = min_n_time) %>%
ungroup()
# Check balance
time_balanced %>%
count(time_of_day)
# Stratified downsampling by weekday
set.seed(123)
min_n_weekday <- tb %>%
count(weekday) %>%
pull(n) %>%
min()
weekday_balanced <- tb %>%
group_by(weekday) %>%
slice_sample(n = min_n_weekday) %>%
ungroup()
# Check balance
weekday_balanced %>%
count(weekday)
# Violin plot
ggplot(time_balanced, aes(x = time_of_day, y = sentiment, fill = time_of_day)) +
geom_violin(trim = FALSE, scale = "width") +
stat_summary(fun = "median", geom = "point", shape = 21, size = 2, fill = "white") +
labs(title = "Sentiment Distribution by Time of Day (Balanced)",
x = "Time of Day",
y = "Sentiment Score") +
theme_minimal() +
theme(legend.position = "none")
time_balanced %>%
group_by(time_of_day) %>%
summarise(
median_sentiment = median(sentiment),
mean_sentiment = mean(sentiment),
n = n()
)
# Kolmogorov-Smirnov test for checking normality with large sample sizes
# Null Hypothesis (H₀): The data follows a normal distribution
# Alternative Hypothesis (H₁):The data does NOT follow a normal distribution
time_balanced %>%
group_by(time_of_day) %>%
summarise(ks_test = ks.test(sentiment, "pnorm", mean(sentiment), sd(sentiment))$p.value)
# Kruskal-Wallis Test
kruskal.test(sentiment ~ time_of_day, data = time_balanced)
# Perform post hoc test
time_pairwise <- pairwise.wilcox.test(time_balanced$sentiment, time_balanced$time_of_day, p.adjust.method = "BH")
time_pairwise
ggplot(weekday_balanced, aes(x = weekday, y = sentiment, fill = weekday)) +
geom_violin(trim = FALSE, scale = "width") +
stat_summary(fun = "median", geom = "point", shape = 21, size = 2, fill = "white") +
labs(title = "Sentiment Distribution by Weekday (Balanced)",
x = "Weekday",
y = "Sentiment Score") +
theme_minimal() +
theme(legend.position = "none")
# Kolmogorov-Smirnov test for checking normality with large sample sizes
# Null Hypothesis (H₀): The data follows a normal distribution
# Alternative Hypothesis (H₁):The data does NOT follow a normal distribution
weekday_balanced %>%
group_by(weekday) %>%
summarise(ks_test = ks.test(sentiment, "pnorm", mean(sentiment), sd(sentiment))$p.value)
# Kruskal-Wallis Test by weekday
kruskal.test(sentiment ~ factor(weekday), data = weekday_balanced)
# Add row ID
time_balanced <- time_balanced %>%
mutate(row_id = row_number())
# Filter morning and remove stop words
morning_words <- time_balanced %>%
filter(time_of_day == "Morning") %>%
select(row_id, text) %>%
unnest_tokens(word, text) %>%
filter(!word %in% expanded_stop_words,
hunspell_check(word, dict = "en_US")) #stop word filter
# Calculate co-occurrence counts (within same post)
morning_pairs <- morning_words %>%
pairwise_count(word, row_id, sort = TRUE, upper = FALSE)
# Filter for strong connections only
morning_pairs_filtered <- morning_pairs %>%
filter(n >= 3)  # Only keep strong links
# Build graph
graph <- morning_pairs_filtered %>%
graph_from_data_frame()
# Plot with ggraph
ggraph(graph, layout = "fr") +
geom_edge_link(aes(edge_alpha = n), show.legend = FALSE) +
geom_node_point(color = "skyblue", size = 5) +
geom_node_text(aes(label = name), repel = TRUE, size = 4) +
labs(title = "Morning Word Co-Occurrence Network") +
theme_void()
# Filter afternoon and remove stop words
afternoon_words <- time_balanced %>%
filter(time_of_day == "Afternoon") %>%
select(row_id, text) %>%
unnest_tokens(word, text) %>%
filter(!word %in% expanded_stop_words,
hunspell_check(word, dict = "en_US")) #stop word filter
# Calculate co-occurrence counts (within same post)
afternoon_pairs <- afternoon_words %>%
pairwise_count(word, row_id, sort = TRUE, upper = FALSE)
# Filter for strong connections only
afternoon_pairs_filtered <- afternoon_pairs %>%
filter(n >= 3)  # Only keep strong links
# Build graph
graph <- afternoon_pairs_filtered %>%
graph_from_data_frame()
# Plot with ggraph
ggraph(graph, layout = "fr") +
geom_edge_link(aes(edge_alpha = n), show.legend = FALSE) +
geom_node_point(color = "skyblue", size = 5) +
geom_node_text(aes(label = name), repel = TRUE, size = 4) +
labs(title = "Afternoon Word Co-Occurrence Network") +
theme_void()
# Filter evening and remove stop words
evening_words <- time_balanced %>%
filter(time_of_day == "Evening") %>%
select(row_id, text) %>%
unnest_tokens(word, text) %>%
filter(!word %in% expanded_stop_words,
hunspell_check(word, dict = "en_US")) #stop word filter
# Calculate co-occurrence counts (within same post)
evening_pairs <- evening_words %>%
pairwise_count(word, row_id, sort = TRUE, upper = FALSE)
# Filter for strong connections only
evening_pairs_filtered <- evening_pairs %>%
filter(n >= 3)  # Only keep strong links
# Build graph
graph <- evening_pairs_filtered %>%
graph_from_data_frame()
# Plot with ggraph
ggraph(graph, layout = "fr") +
geom_edge_link(aes(edge_alpha = n), show.legend = FALSE) +
geom_node_point(color = "skyblue", size = 5) +
geom_node_text(aes(label = name), repel = TRUE, size = 4) +
labs(title = "Evening Word Co-Occurrence Network") +
theme_void()
# Filter night and remove stop words
night_words <- time_balanced %>%
filter(time_of_day == "Night") %>%
select(row_id, text) %>%
unnest_tokens(word, text) %>%
filter(!word %in% expanded_stop_words,
hunspell_check(word, dict = "en_US")) #stop word filter
# Calculate co-occurrence counts (within same post)
night_pairs <- night_words %>%
pairwise_count(word, row_id, sort = TRUE, upper = FALSE)
# Filter for strong connections only
night_pairs_filtered <- night_pairs %>%
filter(n >= 3)  # Only keep strong links
# Build graph
graph <- night_pairs_filtered %>%
graph_from_data_frame()
# Plot with ggraph
ggraph(graph, layout = "fr") +
geom_edge_link(aes(edge_alpha = n), show.legend = FALSE) +
geom_node_point(color = "skyblue", size = 5) +
geom_node_text(aes(label = name), repel = TRUE, size = 4) +
labs(title = "Night Word Co-Occurrence Network") +
theme_void()
# Function to compute network
network_stats_by_time <- function(data, time_label) {
# Filter for one time_of_day group
word_pairs <- data %>%
filter(time_of_day == time_label) %>%
unnest_tokens(word, text) %>%
filter(!word %in% expanded_stop_words,
hunspell_check(word, dict = "en_US")) %>%
pairwise_count(word, row_id, sort = TRUE, upper = FALSE) %>%
filter(n >= 3)
# Build graph
g <- graph_from_data_frame(word_pairs, directed = FALSE)
# Compute metrics
data.frame(
time_of_day = time_label,
nodes = vcount(g),
edges = ecount(g),
density = edge_density(g),
components = components(g)$no,
modularity = modularity(cluster_louvain(g))
)
}
# Apply to all 4 time bins
network_summary <- bind_rows(
network_stats_by_time(time_balanced, "Morning"),
network_stats_by_time(time_balanced, "Afternoon"),
network_stats_by_time(time_balanced, "Evening"),
network_stats_by_time(time_balanced, "Night")
)
network_summary
# Bar chart of modularity
ggplot(network_summary, aes(x = factor(time_of_day, levels = c("Morning", "Afternoon", "Evening", "Night")),
y = modularity, fill = time_of_day)) +
geom_col(show.legend = FALSE) +
labs(title = "Topic Modularity by Time of Day",
x = "Time of Day",
y = "Modularity Score") +
theme_minimal()
# Build co-occurrence network per time bin
build_cooc_graph <- function(df, time_label) {
word_pairs <- df %>%
filter(time_of_day == time_label) %>%
unnest_tokens(word, text) %>%
filter(!word %in% expanded_stop_words,
hunspell_check(word, dict = "en_US")) %>%
pairwise_count(word, row_id, sort = TRUE, upper = FALSE) %>%
filter(n >= 3)
g <- graph_from_data_frame(word_pairs, directed = FALSE)
g <- simplify(g, remove.multiple = TRUE, remove.loops = TRUE)
# Add Louvain communities
V(g)$community <- cluster_louvain(g)$membership
V(g)$name <- V(g)$name
g
}
# Build graphs
g_morning   <- build_cooc_graph(time_balanced, "Morning")
g_afternoon <- build_cooc_graph(time_balanced, "Afternoon")
g_evening   <- build_cooc_graph(time_balanced, "Evening")
g_night     <- build_cooc_graph(time_balanced, "Night")
# Plot function
plot_network <- function(graph, title) {
ggraph(graph, layout = "fr") +
geom_edge_link(alpha = 0.2) +
geom_node_point(aes(color = as.factor(community)), size = 3) +
geom_node_text(aes(label = name), repel = TRUE, size = 3) +
labs(title = title) +
theme_void() +
theme(legend.position = "none")
}
# Generate plots
p1 <- plot_network(g_morning, "Morning")
p2 <- plot_network(g_afternoon, "Afternoon")
p3 <- plot_network(g_evening, "Evening")
p4 <- plot_network(g_night, "Night")
# Combine plots into a 2x2 grid
(p1 | p2) / (p3 | p4)
